{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc1e1f4-a6e2-4fd7-a820-55ee1d34af44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b72d236-44fd-4ab2-9acc-d9a910ba7e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaStructuredStreaming\") \\\n",
    "    .config('spark.default.parallelism', 1) \\\n",
    "    .config('spark.sql.shuffle.partitions', 1) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c349063-6bef-478a-b653-039cc4328b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6b918c-d67c-48ba-91fe-c7ffde2ce2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# For sentence and word tokenize\n",
    "nltk.download('punkt')\n",
    "# For word tagging (pos_tag)\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "# For extracting named entities\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "from functools import reduce\n",
    "\n",
    "def get_entities_count(sentence):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    tagged_words = nltk.pos_tag(tokens)\n",
    "    chunks = nltk.ne_chunk(tagged_words)\n",
    "\n",
    "    c = filter(lambda x: isinstance(x, nltk.Tree), chunks)\n",
    "    leaves = map(lambda x: x.leaves(), c)\n",
    "    entities = reduce(list.__add__, leaves, [])\n",
    "    only_entities = map(lambda x: x[0].lower(), entities)\n",
    "\n",
    "    return list(only_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057caa26-c376-4cdd-ac05-0b0ef7389724",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType\n",
    "from pyspark.sql.functions import from_json, to_json, struct, col, explode, count\n",
    "\n",
    "kafka_bootstrap_servers = \"localhost:9092\"\n",
    "kafka_read_topic = \"topic1\"\n",
    "kafka_write_topic = \"topic2\"\n",
    "schema = StructType([\n",
    "    StructField(\"body\", StringType(), True),\n",
    "    StructField(\"timestamp\", IntegerType(), True)\n",
    "])\n",
    "checkpoint_location = \"./kafka_checkpoints\"\n",
    "\n",
    "map_udf = udf(get_entities_count, ArrayType(StringType(), False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afef419-5481-49d1-90bf-4487b6849f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "    .option(\"subscribe\", kafka_read_topic) \\\n",
    "    .option(\"maxOffsetsPerTrigger\", 1000) \\\n",
    "    .load() \\\n",
    "    .selectExpr(\"CAST(value AS STRING)\") \\\n",
    "    .select(from_json(\"value\", schema).alias(\"data\")).select(\"data.*\") \\\n",
    "    .withColumn(\"entities\", map_udf(\"body\")) \\\n",
    "    .select(explode(\"entities\").alias(\"entity\"), \"timestamp\", \"body\") \\\n",
    "    .groupBy(\"entity\") \\\n",
    "    .agg(count(\"entity\").alias(\"count\")) \\\n",
    "    .select(\"entity\", \"count\") \\\n",
    "    .orderBy(\"count\", ascending=False) \\\n",
    "    .withColumn(\"value\", to_json(struct(col(\"entity\"), col(\"count\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492cc574-0a8f-4114-a6ff-4b240d3f97c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed727925-6d62-45c2-a86b-d61db5cfb83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = streaming_df \\\n",
    "#     .writeStream \\\n",
    "#     .outputMode(\"complete\") \\\n",
    "#     .format(\"console\") \\\n",
    "#     .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0a0928-aa72-427f-aa74-1ba2168799fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d801ab7d-5cc3-409c-877d-c286a65c58f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = streaming_df \\\n",
    "    .repartition(1) \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "    .option(\"topic\", kafka_write_topic) \\\n",
    "    .option(\"checkpointLocation\", checkpoint_location) \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60689709-9519-4cf1-81bf-36a09058bc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start streaming data to topic2\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c9e49c-08ce-4abd-adbd-f7aabf5fdab6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
